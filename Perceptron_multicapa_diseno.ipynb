{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01275ef5-fe77-441d-82fc-2c2473972a0a",
   "metadata": {
    "id": "01275ef5-fe77-441d-82fc-2c2473972a0a"
   },
   "source": [
    "# Clasificaci√≥n de MNIST con un Perceptr√≥n Multicapa (MLP)\n",
    "\n",
    "## Perceptr√≥n Multicapa (MLP)\n",
    "\n",
    "Un **Perceptr√≥n Multicapa (MLP)** es una red neuronal artificial compuesta por varias capas de neuronas. Es uno de los modelos m√°s b√°sicos y cl√°sicos de redes neuronales, utilizado para tareas de clasificaci√≥n y regresi√≥n.\n",
    "\n",
    "###  Estructura del MLP:\n",
    "\n",
    "1. **Capa de entrada (Input layer):**\n",
    "   - Recibe los datos de entrada.\n",
    "   - En el caso del dataset MNIST, cada imagen tiene tama√±o 28x28 p√≠xeles, que se convierte en un vector de 784 elementos.\n",
    "   - Cada elemento representa un p√≠xel con un valor entre 0 y 1 (despu√©s de normalizar).\n",
    "\n",
    "2. **Capas ocultas (Hidden layers):**\n",
    "   - Compuestas por neuronas densamente conectadas (fully connected).\n",
    "   - Usan funciones de activaci√≥n no lineales (como ReLU).\n",
    "   - Permiten aprender representaciones internas complejas.\n",
    "\n",
    "3. **Capa de salida (Output layer):**\n",
    "   - Tiene 10 neuronas, una por cada clase (d√≠gitos del 0 al 9).\n",
    "   - Utiliza activaci√≥n **softmax** para generar una distribuci√≥n de probabilidad.\n",
    "\n",
    "\n",
    "## Dataset MNIST?\n",
    "\n",
    "El dataset MNIST contiene:\n",
    "- 60,000 im√°genes de entrenamiento y 10,000 de prueba.\n",
    "- Cada imagen es de 28x28 p√≠xeles en escala de grises.\n",
    "- Las etiquetas son d√≠gitos del 0 al 9.\n",
    "\n",
    "\n",
    "\n",
    "## Entrenamiento de un MLP\n",
    "\n",
    "El entrenamiento sigue un ciclo de pasos iterativos para ajustar los pesos de la red y minimizar el error en las predicciones.\n",
    "\n",
    "### 1. **Propagaci√≥n hacia adelante (Forward pass)**\n",
    "\n",
    "Se calculan las salidas de cada capa de la red usando los pesos actuales:\n",
    "\n",
    "$$\n",
    "\\text{z}^{(1)} = W^{(1)}x + b^{(1)} \\quad \\text{‚Üí entrada a la capa oculta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{a}^{(1)} = \\text{ReLU}(\\text{z}^{(1)}) \\quad \\text{‚Üí salida de la capa oculta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{z}^{(2)} = W^{(2)}a^{(1)} + b^{(2)} \\quad \\text{‚Üí entrada a la capa de salida}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z^{(2)}) \\quad \\text{‚Üí probabilidades para cada clase}\n",
    "$$\n",
    "\n",
    "### 2. **C√°lculo de la p√©rdida (Loss function)**\n",
    "\n",
    "Se compara la predicci√≥n del modelo con la etiqueta real mediante una funci√≥n de p√©rdida. En clasificaci√≥n multiclase, se usa la **entrop√≠a cruzada (cross-entropy)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{10} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ y_i $ es la etiqueta real (en one-hot encoding).\n",
    "- $ \\hat{y}_i $ es la probabilidad predicha para la clase $i $.\n",
    "\n",
    "### 3. **Retropropagaci√≥n y ajuste de pesos (Backpropagation + Gradient Descent)**\n",
    "\n",
    "Despu√©s de calcular la p√©rdida, necesitamos ajustar los pesos para reducir el error. Esto se hace en varios pasos:\n",
    "\n",
    "#### a) Error en la capa de salida\n",
    "\n",
    "El error que tiene la red en la salida es la diferencia entre la predicci√≥n y la etiqueta real:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Aqu√≠:\n",
    "- $\\delta^{(2)}$ es el vector de errores para la capa de salida.\n",
    "- $\\hat{y}$ es la salida predicha para cada clase.\n",
    "- $y$ es la etiqueta verdadera.\n",
    "\n",
    "#### b) Error en la capa oculta\n",
    "\n",
    "Este error se propaga hacia atr√°s para saber cu√°nto influy√≥ cada neurona oculta en el error final:\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\circ \\text{ReLU}'(z^{(1)})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $(W^{(2)})^T$ es la matriz de pesos de la capa de salida transpuesta.\n",
    "- $\\circ$ indica multiplicaci√≥n elemento a elemento.\n",
    "- $\\text{ReLU}'(z^{(1)})$ es la derivada de ReLU, que vale 1 si $z^{(1)} > 0$, y 0 si no.\n",
    "\n",
    "#### c) C√°lculo de gradientes para pesos y sesgos\n",
    "\n",
    "Con los errores calculados, ahora determinamos cu√°nto cambiar cada peso y sesgo para mejorar la predicci√≥n.\n",
    "\n",
    "- Gradiente para pesos de la capa de salida:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\delta^{(2)} (a^{(1)})^T\n",
    "$$\n",
    "\n",
    "- Gradiente para sesgos de la capa de salida:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "- Gradiente para pesos de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\delta^{(1)} (x)^T\n",
    "$$\n",
    "\n",
    "- Gradiente para sesgos de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "#### d) Actualizaci√≥n de pesos y sesgos\n",
    "\n",
    "Finalmente, ajustamos los pesos y sesgos usando la tasa de aprendizaje $\\eta$:\n",
    "\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{(l)} := b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "con $l = 1, 2$ indicando la capa (oculta o salida).cto a ese peso.\n",
    "\n",
    "\n",
    "###  Entrenamiento por lotes y √©pocas\n",
    "\n",
    "- El conjunto de datos se divide en lotes (batches).\n",
    "- Cada lote se usa para actualizar los pesos una vez.\n",
    "- Una **√©poca** es un recorrido completo por todo el conjunto de entrenamiento.\n",
    "- El modelo mejora iterativamente al ver m√°s datos.\n",
    "\n",
    "\n",
    "\n",
    "###  ¬øQu√© aprende el modelo?\n",
    "\n",
    "- Comienza con pesos aleatorios.\n",
    "- A trav√©s del entrenamiento, ajusta los pesos para minimizar el error.\n",
    "- Aprende a asociar patrones visuales (p√≠xeles) con etiquetas (n√∫meros).\n",
    "- El objetivo es **generalizar bien** para predecir correctamente im√°genes no vistas.\n",
    "\n",
    "\n",
    "Este proceso permite que un MLP aprenda a clasificar im√°genes de d√≠gitos escritos a mano con alta precisi√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f14fc-3808-4f76-a638-e008d3ce02d6",
   "metadata": {
    "id": "cf0f14fc-3808-4f76-a638-e008d3ce02d6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar p√≠xeles\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "#  Learning rate\n",
    "learning_rate_opt = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_opt)\n",
    "\n",
    "#  Modelo MLP con 124 neuronas (√≥ptimo)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar con optimizador modificado\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # <-- Optimizer con tasa de aprendizaje √≥ptima\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Graficar p√©rdida\n",
    "plt.plot(history.history['loss'], label='P√©rdida entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='P√©rdida validaci√≥n')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.legend()\n",
    "plt.title('Convergencia de la funci√≥n de p√©rdida')\n",
    "plt.show()\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Reporte sklearn\n",
    "print(\"\\nReporte de clasificaci√≥n (precision, recall, f1-score):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusi√≥n:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0860e05-9650-4f23-bc0b-5fe0b21e3e16",
   "metadata": {
    "id": "a0860e05-9650-4f23-bc0b-5fe0b21e3e16"
   },
   "source": [
    "# Dise√±o de Experimentos para un Perceptr√≥n Multicapa (MLP)\n",
    "\n",
    "## 1. Define el objetivo del experimento\n",
    "\n",
    "- Optimizar la arquitectura (n√∫mero de capas, neuronas).\n",
    "- Optimizar hiperpar√°metros de entrenamiento (learning rate, batch size, regularizaci√≥n).\n",
    "- Comparar funciones de activaci√≥n o tipos de optimizadores.\n",
    "\n",
    "## 2. Tipos de factores\n",
    "\n",
    "- **Continuos:** learning rate, momentum, dropout rate, n√∫mero de neuronas (discreto con muchos niveles).\n",
    "- **Discretos:** n√∫mero de capas, funciones de activaci√≥n, optimizadores.\n",
    "- **Categ√≥ricos:** tipos de regularizaci√≥n, funciones de activaci√≥n, optimizador.\n",
    "\n",
    "## 3. Dise√±os experimentales comunes para MLP\n",
    "\n",
    "| M√©todo                        | Descripci√≥n                                               | Ventajas                                | Limitaciones                         |\n",
    "|-------------------------------|-----------------------------------------------------------|----------------------------------------|-------------------------------------|\n",
    "| **B√∫squeda en malla (Grid Search)**        | Probar combinaciones en rejilla regular.                 | F√°cil de implementar, sistem√°tico.    | Costoso, explora pobremente el espacio. |\n",
    "| **B√∫squeda aleatoria (Random Search)**     | Probar puntos aleatorios en el espacio.                   | M√°s eficiente que grid, f√°cil.         | No garantiza cobertura completa.   |\n",
    "| **Optimizaci√≥n bayesiana**                  | Modela la funci√≥n objetivo con un modelo probabil√≠stico. | Muy eficiente, menos ejecuciones.      | M√°s compleja, requiere librer√≠as.   |\n",
    "| **Dise√±o de Superficie de Respuesta (RSM)**| Usa dise√±os factoriales y centrales para ajustar modelo. | Balance entre exploraci√≥n y modelado.  | Requiere funci√≥n respuesta suave, pocos factores. |\n",
    "| **Dise√±o factorial fraccional**             | Explora combinaciones seleccionadas para reducir pruebas.| Reduce experimentos, explora interacciones principales. | Menos exhaustivo, puede perder interacciones. |\n",
    "| **Algoritmos heur√≠sticos**             | Son una combinaci√≥n entre exploraci√≥n y explotaci√≥n del espacio.| Puede explorar mas parte del espacio. | En ocasiones puede ser costoso computacionalmente. |\n",
    "\n",
    "## 4. Recomendaciones pr√°cticas\n",
    "\n",
    "- Para pocos factores continuos: **RSM con dise√±o Central Compuesto (CCD)** para entender efectos e interacciones.\n",
    "- Para muchos hiperpar√°metros o combinaciones: **b√∫squeda aleatoria o bayesiana**.\n",
    "- Usar siempre conjunto de validaci√≥n para medir desempe√±o.\n",
    "- Realizar r√©plicas en condiciones centrales para estimar variabilidad.\n",
    "\n",
    "## 5. Variables recomendadas a experimentar en un MLP\n",
    "\n",
    "- Learning rate (continuo)\n",
    "- N√∫mero de neuronas por capa (discreto)\n",
    "- N√∫mero de capas (discreto)\n",
    "- Batch size (discreto)\n",
    "- Regularizaci√≥n (L2 lambda, dropout rate)\n",
    "- Funci√≥n de activaci√≥n (ReLU, tanh, sigmoid)\n",
    "\n",
    "## 6. Ejemplo √≥ptimo para un MLP simple\n",
    "\n",
    "- Para optimizar learning rate, neuronas y batch size: **Dise√±o RSM** es ideal.\n",
    "- Para probar arquitecturas y optimizadores: combinar dise√±o factorial fraccional (variables categ√≥ricas) y RSM o b√∫squeda aleatoria (variables continuas).\n",
    "\n",
    "\n",
    "\n",
    "## Resumen: ¬øCu√°l es la mejor forma?\n",
    "\n",
    "- **Pocos factores num√©ricos:** Dise√±o Central Compuesto (CCD) con RSM.\n",
    "- **Muchos hiperpar√°metros:** Optimizaci√≥n bayesiana o b√∫squeda aleatoria.\n",
    "- **Experimentos r√°pidos:** Grid search con validaci√≥n cruzada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f380584-125c-43b8-9748-05598acb85fa",
   "metadata": {
    "id": "0f380584-125c-43b8-9748-05598acb85fa"
   },
   "source": [
    "# Dise√±o Central Compuesto (CCD) y M√©todo de Superficie de Respuesta (RSM)\n",
    "\n",
    "## Introducci√≥n al M√©todo de Superficie de Respuesta (RSM)\n",
    "\n",
    "El M√©todo de Superficie de Respuesta (Response Surface Methodology, RSM) es un conjunto de t√©cnicas estad√≠sticas para modelar y analizar problemas en los cuales una o m√°s variables de respuesta $y$ dependen de varias variables independientes o factores $x_1, x_2, ..., x_k$.\n",
    "\n",
    "El objetivo principal del RSM es:\n",
    "\n",
    "- Encontrar las condiciones √≥ptimas de los factores que maximicen o minimicen la respuesta.\n",
    "- Modelar la relaci√≥n entre las variables de entrada y la respuesta mediante funciones matem√°ticas simples, generalmente polinomios de segundo grado (cuadr√°ticos).\n",
    "\n",
    "\n",
    "##  Dise√±o Central Compuesto (CCD)\n",
    "\n",
    "El Dise√±o Central Compuesto (Central Composite Design, CCD) es uno de los dise√±os experimentales m√°s usados para ajustar modelos de superficie de respuesta de segundo orden (cuadr√°ticos). Es ideal cuando se tienen pocos factores num√©ricos y se desea explorar no solo los efectos lineales sino tambi√©n los efectos cuadr√°ticos y las interacciones entre factores.\n",
    "\n",
    "### Componentes del CCD:\n",
    "\n",
    "- **Puntos factoriales:** Combinaciones de los niveles altos (+1) y bajos (-1) para cada factor, formando un dise√±o factorial completo ($2^k$ puntos para $k$ factores).\n",
    "- **Puntos axiales (puntos estrella):** Extienden el rango de exploraci√≥n fuera del dise√±o factorial con niveles a distancia $\\pm \\alpha$ en cada factor, manteniendo los dem√°s en nivel central (0). Estos puntos permiten estimar efectos cuadr√°ticos.\n",
    "- **Puntos centrales:** Uno o m√°s puntos en el centro del dise√±o (nivel 0 para todos los factores) que ayudan a estimar la variabilidad experimental y la curvatura.\n",
    "\n",
    "### Par√°metro $\\alpha$:\n",
    "\n",
    "El valor de $\\alpha$ define la distancia de los puntos axiales al centro y puede elegirse para mantener propiedades estad√≠sticas espec√≠ficas, como la rotabilidad (igual precisi√≥n en todas las direcciones).\n",
    "\n",
    "\n",
    "\n",
    "## Codificaci√≥n de factores\n",
    "\n",
    "Para facilitar el an√°lisis, los niveles de los factores se codifican de forma estandarizada:\n",
    "\n",
    "$$\n",
    "x_i = \\frac{\\text{valor real} - \\text{valor central}}{\\text{semi-rango}}\n",
    "$$\n",
    "donde:\n",
    "\n",
    "- $x_i$ es el nivel codificado del factor $i$, t√≠picamente en el rango $[-1, +1]$ para los puntos factoriales.\n",
    "- El valor central es el punto medio del rango real del factor.\n",
    "- El semi-rango es la mitad del rango real.\n",
    "\n",
    "Esta codificaci√≥n permite comparar efectos de diferentes factores en una escala com√∫n.\n",
    "\n",
    "\n",
    "\n",
    "##  Ajuste del modelo de superficie de respuesta\n",
    "\n",
    "Con los datos obtenidos del experimento CCD, se ajusta un modelo polinomial cuadr√°tico de la forma:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\sum_{i=1}^k \\beta_i x_i + \\sum_{i=1}^k \\beta_{ii} x_i^2 + \\sum_{i<j} \\beta_{ij} x_i x_j + \\varepsilon\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $y$ es la variable respuesta (p. ej., precisi√≥n del modelo MLP).\n",
    "- $\\beta_0$ es la ordenada al origen.\n",
    "- $\\beta_i$, $\\beta_{ii}$, y $\\beta_{ij}$ son los coeficientes que representan los efectos lineales, cuadr√°ticos e interacci√≥n, respectivamente.\n",
    "- $\\varepsilon$ es el t√©rmino de error experimental.\n",
    "\n",
    "Este modelo permite:\n",
    "\n",
    "- Identificar los efectos significativos de cada factor y sus interacciones.\n",
    "- Visualizar la superficie de respuesta.\n",
    "- Encontrar los valores √≥ptimos de los factores que maximizan o minimizan $y$.\n",
    "\n",
    "\n",
    "##  Aplicaci√≥n pr√°ctica en MLP\n",
    "\n",
    "En el contexto de un Perceptr√≥n Multicapa (MLP):\n",
    "\n",
    "- Los factores pueden ser hiperpar√°metros como tasa de aprendizaje, n√∫mero de neuronas, n√∫mero de capas, etc.\n",
    "- La respuesta puede ser la precisi√≥n, error de validaci√≥n o cualquier m√©trica de desempe√±o.\n",
    "- Utilizando CCD y RSM, podemos dise√±ar un conjunto reducido pero eficiente de experimentos para explorar c√≥mo los hiperpar√°metros afectan el desempe√±o del MLP y encontrar configuraciones √≥ptimas sin realizar b√∫squedas exhaustivas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72c452-5ee0-439f-979c-4eaea8434cd4",
   "metadata": {
    "id": "de72c452-5ee0-439f-979c-4eaea8434cd4"
   },
   "source": [
    "# Optimizaci√≥n de un Perceptr√≥n Multicapa (MLP) con Superficie de Respuesta (RSM) y Dise√±o Central Compuesto (CCD)\n",
    "\n",
    "##  Objetivo\n",
    "\n",
    "Optimizar el rendimiento de un Perceptr√≥n Multicapa (MLP) ajustando dos hiperpar√°metros clave:\n",
    "\n",
    "- **Tasa de aprendizaje** (`learning_rate`)\n",
    "- **N√∫mero de neuronas ocultas** (`units`)\n",
    "\n",
    "Usamos el **M√©todo de Superficie de Respuesta (RSM)** con un **Dise√±o Central Compuesto (CCD)** para construir un modelo cuadr√°tico que relacione estos hiperpar√°metros con el desempe√±o del MLP (precisi√≥n de validaci√≥n).\n",
    "\n",
    "\n",
    "##  Paso 1: Selecci√≥n de factores y niveles\n",
    "\n",
    "Seleccionamos dos factores, cada uno con 5 niveles: codificados como $- \\alpha, -1, 0, +1, +\\alpha$, donde $\\alpha = \\sqrt{2} \\approx 1.414$ (valor com√∫n para dos factores).\n",
    "\n",
    "###  Factores y niveles reales\n",
    "\n",
    "| Factor              | $-\\alpha$  | $-1$      | $0$       | $+1$      | $+\\alpha$  |\n",
    "|---------------------|------------|-----------|-----------|-----------|------------|\n",
    "| Learning rate       | 0.00001    | 0.0001    | 0.001     | 0.01      | 0.1        |\n",
    "| Neuronas ocultas    | 32         | 64        | 128       | 256       | 512        |\n",
    "\n",
    "###  Puntos del dise√±o CCD\n",
    "\n",
    "El dise√±o CCD incluye:\n",
    "\n",
    "- **4 puntos factoriales**: combinaciones de niveles $-1$ y $+1$\n",
    "- **4 puntos axiales**: niveles extremos $- \\alpha$ y $+ \\alpha$ con los dem√°s factores en 0\n",
    "- **5 r√©plicas del punto central**: todos los factores en nivel 0\n",
    "\n",
    "Esto da un total de **13 experimentos**.\n",
    "\n",
    "| Punto | $x_1$ (codificado) | $x_2$ (codificado) | learning\\_rate | units |\n",
    "|-------|--------------------|--------------------|----------------|--------|\n",
    "| F1    | -1                 | -1                 | 0.0001         | 64     |\n",
    "| F2    | +1                 | -1                 | 0.01           | 64     |\n",
    "| F3    | -1                 | +1                 | 0.0001         | 256    |\n",
    "| F4    | +1                 | +1                 | 0.01           | 256    |\n",
    "| A1    | -1.414             | 0                  | 0.00004        | 128    |\n",
    "| A2    | +1.414             | 0                  | 0.0252         | 128    |\n",
    "| A3    | 0                  | -1.414             | 0.001          | 45     |\n",
    "| A4    | 0                  | +1.414             | 0.001          | 362    |\n",
    "| C1    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C2    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C3    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C4    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C5    | 0                  | 0                  | 0.001          | 128    |\n",
    "\n",
    "##  Paso 2: Modelo cuadr√°tico de respuesta\n",
    "\n",
    "Usamos un modelo cuadr√°tico de segundo orden:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $y$ es la precisi√≥n de validaci√≥n del MLP\n",
    "- $x_1$: nivel codificado del `learning_rate`\n",
    "- $x_2$: nivel codificado del n√∫mero de `units`\n",
    "- $\\beta$ son los coeficientes a estimar\n",
    "- $\\varepsilon$ es el error aleatorio\n",
    "\n",
    "Este modelo permite capturar efectos lineales, cuadr√°ticos y de interacci√≥n entre los hiperpar√°metros.\n",
    "\n",
    "\n",
    "## Paso 3: Ejecuci√≥n del experimento\n",
    "\n",
    "1. **Construir la matriz de dise√±o CCD** con los niveles codificados ($x_1$, $x_2$).\n",
    "2. **Convertir los niveles codificados** a valores reales de hiperpar√°metros.\n",
    "3. **Entrenar el MLP** con cada combinaci√≥n y registrar la precisi√≥n de validaci√≥n.\n",
    "4. **Ajustar el modelo cuadr√°tico** usando regresi√≥n lineal (con `statsmodels` o `sklearn.linear_model.LinearRegression`).\n",
    "5. **Analizar los resultados**:\n",
    "   - Significancia de los t√©rminos (lineales, cuadr√°ticos, interacci√≥n)\n",
    "   - Superficie de respuesta (mapa 3D o contornos)\n",
    "   - Hiperpar√°metros √≥ptimos\n",
    "\n",
    "\n",
    "## Ventajas del enfoque\n",
    "\n",
    "- Eficiencia: Menos combinaciones que una b√∫squeda en malla (grid search).\n",
    "- Modelo explicativo y predictivo del desempe√±o del MLP.\n",
    "- Posibilidad de visualizar e interpretar la superficie de respuesta.\n",
    "- Permite evaluar **efectos individuales y combinados** de los hiperpar√°metros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a72d3a1-0bd1-4dd2-a84a-10d3d6df116a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a72d3a1-0bd1-4dd2-a84a-10d3d6df116a",
    "outputId": "2189a016-2a22-45d7-e49d-d1a8b835f6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Ejecutando experimento 1: lr=0.00010, units=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando experimento 2: lr=0.01000, units=64\n",
      "Ejecutando experimento 3: lr=0.00010, units=256\n",
      "Ejecutando experimento 4: lr=0.01000, units=256\n",
      "Ejecutando experimento 5: lr=0.00004, units=128\n",
      "Ejecutando experimento 6: lr=0.02594, units=128\n",
      "Ejecutando experimento 7: lr=0.00100, units=48\n",
      "Ejecutando experimento 8: lr=0.00100, units=341\n",
      "Ejecutando experimento 9: lr=0.00100, units=128\n",
      "Ejecutando experimento 10: lr=0.00100, units=128\n",
      "Ejecutando experimento 11: lr=0.00100, units=128\n",
      "Ejecutando experimento 12: lr=0.00100, units=128\n",
      "Ejecutando experimento 13: lr=0.00100, units=128\n",
      "       x1     x2  learning_rate  units  val_accuracy\n",
      "0  -1.000 -1.000       0.000100     64      0.940417\n",
      "1   1.000 -1.000       0.010000     64      0.966667\n",
      "2  -1.000  1.000       0.000100    256      0.960917\n",
      "3   1.000  1.000       0.010000    256      0.967417\n",
      "4  -1.414  0.000       0.000039    128      0.929833\n",
      "5   1.414  0.000       0.025942    128      0.943750\n",
      "6   0.000 -1.414       0.001000     48      0.966750\n",
      "7   0.000  1.414       0.001000    341      0.981667\n",
      "8   0.000  0.000       0.001000    128      0.975917\n",
      "9   0.000  0.000       0.001000    128      0.974833\n",
      "10  0.000  0.000       0.001000    128      0.976083\n",
      "11  0.000  0.000       0.001000    128      0.973833\n",
      "12  0.000  0.000       0.001000    128      0.974917\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar y normalizar los datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Dividir entrenamiento en entrenamiento + validaci√≥n\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Funci√≥n de entrenamiento\n",
    "# -----------------------------\n",
    "def train_model(learning_rate, units):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Devolver √∫ltima precisi√≥n de validaci√≥n\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# -----------------------------\n",
    "# Tabla del dise√±o CCD\n",
    "# -----------------------------\n",
    "design = pd.DataFrame([\n",
    "    [-1, -1], [1, -1], [-1, 1], [1, 1],     # factorial\n",
    "    [-1.414, 0], [1.414, 0], [0, -1.414], [0, 1.414],  # axial\n",
    "    [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]  # centro (5 rep)\n",
    "], columns=[\"x1\", \"x2\"])\n",
    "\n",
    "# Conversi√≥n a hiperpar√°metros reales\n",
    "def coded_to_real(x1, x2):\n",
    "    # learning_rate en escala log10\n",
    "    learning_rate = 0.001 * (10 ** x1)\n",
    "    units = int(round(128 * (2 ** x2)))\n",
    "    return learning_rate, units\n",
    "\n",
    "# -----------------------------\n",
    "# Entrenamiento en cada punto\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for i, row in design.iterrows():\n",
    "    x1, x2 = row[\"x1\"], row[\"x2\"]\n",
    "    lr, u = coded_to_real(x1, x2)\n",
    "\n",
    "    print(f\"Ejecutando experimento {i+1}: lr={lr:.5f}, units={u}\")\n",
    "    acc = train_model(lr, u)\n",
    "\n",
    "    results.append({\n",
    "        \"x1\": x1,\n",
    "        \"x2\": x2,\n",
    "        \"learning_rate\": lr,\n",
    "        \"units\": u,\n",
    "        \"val_accuracy\": acc\n",
    "    })\n",
    "\n",
    "# Convertir a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0312-ec0e-45b5-bb25-64a2ad0ba03b",
   "metadata": {
    "id": "81ee0312-ec0e-45b5-bb25-64a2ad0ba03b"
   },
   "source": [
    "## Paso 4: Ajuste del modelo cuadr√°tico de segundo orden\n",
    "Una vez entrenado el perceptr√≥n para cada combinaci√≥n de hiperpar√°metros del dise√±o CCD y registrada la precisi√≥n de validaci√≥n, queremos ajustar un **modelo de segundo orden** que relacione los factores con la respuesta.\n",
    "\n",
    "El modelo que queremos ajustar es:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $y$ es la **precisi√≥n de validaci√≥n**.\n",
    "- $x_1$ es el **nivel codificado del learning rate**.\n",
    "- $x_2$ es el **nivel codificado del n√∫mero de unidades**.\n",
    "- Los $\\beta$ son los coeficientes que se estimar√°n.\n",
    "- $\\varepsilon$ es el error aleatorio (ruido experimental).\n",
    "\n",
    "- **$\\beta_0$**: intercepto (respuesta en el punto central).\n",
    "- **$\\beta_1$ y $\\beta_2$**: efectos lineales de los factores.\n",
    "- **$\\beta_{11}$ y $\\beta_{22}$**: efectos cuadr√°ticos (curvatura) de cada factor.\n",
    "- **$\\beta_{12}$**: interacci√≥n entre factores (c√≥mo cambia el efecto de un factor cuando el otro tambi√©n cambia).\n",
    "\n",
    "\n",
    "###  Procedimiento :\n",
    "\n",
    "1. **Codificaci√≥n de niveles**:\n",
    "   - Convertimos cada combinaci√≥n real de hiperpar√°metros (learning rate, unidades) a sus niveles codificados: $x_1, x_2$.\n",
    "   - Esto se hace con la f√≥rmula:\n",
    "\n",
    "   $$\n",
    "   x_i = \\frac{z_i - z_{i,0}}{\\Delta z_i}\n",
    "   $$\n",
    "\n",
    "   donde $z_i$ es el valor real del factor, $z_{i,0}$ es el valor central, y $\\Delta z_i$ es el paso entre niveles (diferencia entre centro y alto o centro y bajo).\n",
    "\n",
    "2. **Creaci√≥n de variables del modelo**:\n",
    "   - A partir de los niveles codificados, construimos las variables del modelo: $x_1$, $x_2$, $x_1^2$, $x_2^2$, y $x_1x_2$.\n",
    "\n",
    "3. **Ajuste con regresi√≥n**:\n",
    "   - Usamos el paquete `statsmodels` para ajustar un modelo de regresi√≥n lineal m√∫ltiple con estos t√©rminos como predictores y la precisi√≥n como variable respuesta.\n",
    "\n",
    "4. **Evaluaci√≥n del modelo**:\n",
    "   - Obtenemos los coeficientes estimados $\\hat{\\beta}$.\n",
    "   - Evaluamos la significancia estad√≠stica (p-valores) y el coeficiente de determinaci√≥n ($R^2$) para ver qu√© tan bien ajusta el modelo.\n",
    "\n",
    "5. **Uso del modelo**:\n",
    "   - El modelo se puede usar para:\n",
    "     - Predecir la precisi√≥n esperada dados nuevos valores de hiperpar√°metros (dentro del rango).\n",
    "     - Visualizar la **superficie de respuesta**.\n",
    "     - Encontrar la combinaci√≥n √≥ptima que maximiza la precisi√≥n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d350bec7-3ec0-429a-9506-e978cebe20bf",
   "metadata": {
    "id": "d350bec7-3ec0-429a-9506-e978cebe20bf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1. Definir niveles reales y codificados\n",
    "center = {'lr': 0.001, 'units': 128}\n",
    "step = {'lr': 0.0009, 'units': 64}  # (alto - centro)\n",
    "alpha = 1.414  # valor axial\n",
    "\n",
    "# Niveles codificados para CCD\n",
    "ccd_points = [\n",
    "    [-1, -1], [-1,  1], [1, -1], [1,  1],  # factorial\n",
    "    [-alpha, 0], [alpha, 0], [0, -alpha], [0, alpha],  # axiales\n",
    "    [0, 0], [0, 0], [0, 0]  # 3 repeticiones del centro\n",
    "]\n",
    "\n",
    "# Convertimos a hiperpar√°metros reales\n",
    "def decode_level(lr_lvl, unit_lvl):\n",
    "    lr_real = center['lr'] + lr_lvl * step['lr']\n",
    "    units_real = int(center['units'] + unit_lvl * step['units'])\n",
    "    return lr_real, units_real\n",
    "\n",
    "real_hyperparams = [decode_level(lr, u) for lr, u in ccd_points]\n",
    "\n",
    "# 2. Cargar y normalizar MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_train[-12000:]\n",
    "y_val = y_train[-12000:]\n",
    "x_train = x_train[:-12000]\n",
    "y_train = y_train[:-12000]\n",
    "\n",
    "# 3. Entrenar modelo para cada combinaci√≥n\n",
    "accuracies = []\n",
    "for i, (lr, units) in enumerate(real_hyperparams):\n",
    "    print(f\"üîÅ Iteraci√≥n {i+1}: learning_rate={lr}, units={units}\")\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "    loss, acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# 4. Construir variables del modelo cuadr√°tico\n",
    "design_df = pd.DataFrame(ccd_points, columns=['x1', 'x2'])  # niveles codificados\n",
    "design_df['x1^2'] = design_df['x1'] ** 2\n",
    "design_df['x2^2'] = design_df['x2'] ** 2\n",
    "design_df['x1*x2'] = design_df['x1'] * design_df['x2']\n",
    "design_df['accuracy'] = accuracies\n",
    "\n",
    "# 5. Ajustar modelo con statsmodels\n",
    "X = design_df[['x1', 'x2', 'x1^2', 'x2^2', 'x1*x2']]\n",
    "X = sm.add_constant(X)\n",
    "y = design_df['accuracy']\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775c31b-71a7-42a5-8367-224e9e953594",
   "metadata": {
    "id": "b775c31b-71a7-42a5-8367-224e9e953594"
   },
   "source": [
    "### Evaluaci√≥n global del modelo\n",
    "\n",
    "| M√©trica               | Valor      | Interpretaci√≥n |\n",
    "|------------------------|------------|----------------|\n",
    "| **R-squared**          | 0.628      | El modelo explica el **62.8% de la variabilidad** observada en la precisi√≥n. Es aceptable pero no excelente. |\n",
    "| **Adj. R-squared**     | 0.255      | Al ajustar por el n√∫mero de predictores, la varianza explicada baja a **25.5%**, indicando que algunos t√©rminos podr√≠an **no contribuir significativamente**. |\n",
    "| **F-statistic**        | 1.685      | La prueba F eval√∫a si el modelo completo es mejor que uno sin predictores. |\n",
    "| **Prob(F-statistic)**  | 0.291      | No significativo (p > 0.05). **No hay evidencia estad√≠stica de que el modelo en conjunto sea √∫til**. |\n",
    "| **N¬∞ Observaciones**   | 11         | El tama√±o muestral es peque√±o, lo que puede reducir la precisi√≥n de los coeficientes estimados. |\n",
    "\n",
    "\n",
    "### Interpretaci√≥n de coeficientes\n",
    "\n",
    "El modelo ajustado fue:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\n",
    "$$\n",
    "\n",
    "| T√©rmino     | Coef. | p-valor | Interpretaci√≥n |\n",
    "|-------------|-------|---------|----------------|\n",
    "| **const**   | 0.971 | 0.001   | Precisi√≥n esperada en el punto central del dise√±o. Muy significativa. |\n",
    "| **$x_1$** (lineal) | 0.161 | 0.097   | Efecto lineal positivo del learning rate. Aunque no es significativo (p > 0.05), es el m√°s cercano a serlo. |\n",
    "| **$x_2$** (lineal) | 0.0058 | 0.944  | El n√∫mero de neuronas ocultas **no tiene efecto lineal significativo**. |\n",
    "| **$x_1^2$** (cuadr√°tico) | -0.165 | 0.138  | Indica curvatura descendente: puede existir un m√°ximo de precisi√≥n para un valor intermedio de $x_1$, pero no es concluyente. |\n",
    "| **$x_2^2$** (cuadr√°tico) | 0.0475 | 0.634  | No hay evidencia de curvatura respecto al n√∫mero de neuronas ocultas. |\n",
    "| **$x_1 \\cdot x_2$** (interacci√≥n) | -0.0034 | 0.977 | La interacci√≥n entre factores es pr√°cticamente nula. |\n",
    "\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- **El √∫nico efecto con cierto impacto es el t√©rmino lineal de $x_1$ (learning rate)**, pero no alcanza significancia estad√≠stica.\n",
    "- **El n√∫mero de neuronas ocultas no parece influir significativamente** en la precisi√≥n del MLP, al menos dentro del rango analizado.\n",
    "- **No se detecta interacci√≥n significativa** entre ambos factores.\n",
    "- El modelo puede **beneficiarse de simplificaci√≥n** (eliminando t√©rminos no significativos), o de **mayor cantidad de datos** (m√°s r√©plicas o puntos del dise√±o) para mejorar la confiabilidad.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f78e4f-8d37-4e03-9062-0b1ebd12ca62",
   "metadata": {
    "id": "20f78e4f-8d37-4e03-9062-0b1ebd12ca62"
   },
   "source": [
    "## Derivaci√≥n del Punto √ìptimo de la Superficie de Respuesta\n",
    "\n",
    "Queremos encontrar el punto donde la precisi√≥n del MLP es m√°xima, usando el modelo cuadr√°tico ajustado mediante el M√©todo de Superficie de Respuesta (RSM). El modelo ajustado es:\n",
    "\n",
    "$$\n",
    "y(x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\n",
    "$$\n",
    "\n",
    "### Modelo con coeficientes ajustados\n",
    "\n",
    "Sustituimos los coeficientes obtenidos del ajuste del modelo (OLS):\n",
    "\n",
    "$$\n",
    "y(x_1, x_2) = 0.9714 + 0.1607 x_1 + 0.0058 x_2 - 0.1655 x_1^2 + 0.0475 x_2^2 - 0.0034 x_1 x_2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Derivadas parciales\n",
    "\n",
    "Para encontrar el punto √≥ptimo, derivamos parcialmente respecto a $x_1$ y $x_2$, e igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = \\beta_1 + 2\\beta_{11} x_1 + \\beta_{12} x_2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_2} = \\beta_2 + 2\\beta_{22} x_2 + \\beta_{12} x_1 = 0\n",
    "$$\n",
    "\n",
    "Sustituimos los valores num√©ricos:\n",
    "\n",
    "**Ecuaci√≥n 1:**\n",
    "\n",
    "$$\n",
    "0.1607 - 2(0.1655) x_1 - 0.0034 x_2 = 0\n",
    "$$\n",
    "\n",
    "**Ecuaci√≥n 2:**\n",
    "\n",
    "$$\n",
    "0.0058 + 2(0.0475) x_2 - 0.0034 x_1 = 0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Sistema de ecuaciones lineales\n",
    "\n",
    "Simplificamos:\n",
    "\n",
    "1. $-0.331 x_1 - 0.0034 x_2 = -0.1607$  \n",
    "2. $-0.0034 x_1 + 0.095 x_2 = -0.0058$\n",
    "\n",
    "\n",
    "\n",
    "### Soluci√≥n del sistema\n",
    "\n",
    "Resolviendo el sistema se obtiene:\n",
    "\n",
    "- $x_1^* = 0.486$  \n",
    "- $x_2^* = -0.044$\n",
    "\n",
    "Este es el **punto √≥ptimo en coordenadas codificadas**.\n",
    "\n",
    "\n",
    "###  Conversi√≥n a valores reales\n",
    "\n",
    "Usamos la f√≥rmula de transformaci√≥n de coordenadas codificadas a reales:\n",
    "\n",
    "$$\n",
    "x_{\\text{real}} = x_{\\text{center}} + x^* \\cdot \\frac{x_{\\text{high}} - x_{\\text{low}}}{2}\n",
    "$$\n",
    "\n",
    "- Para **learning rate** ($x_1$):  \n",
    "  $0.001 + 0.486 \\cdot \\frac{0.01 - 0.0001}{2} \\approx 0.0034$\n",
    "\n",
    "- Para **units** ($x_2$):  \n",
    "  $128 + (-0.044) \\cdot \\frac{256 - 64}{2} \\approx 124$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc6292-c4e2-4308-b799-23531f7ab80c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54dc6292-c4e2-4308-b799-23531f7ab80c",
    "outputId": "781d2405-e29a-4cb9-f528-8ac4c96083d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 - 6s - 7ms/step - accuracy: 0.9268 - loss: 0.2430 - val_accuracy: 0.9560 - val_loss: 0.1526\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar p√≠xeles\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "#  Learning rate √≥ptimo\n",
    "learning_rate_opt = 0.0034\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_opt)\n",
    "\n",
    "#  Modelo MLP con 124 neuronas (√≥ptimo)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(124, activation='relu'),  # <-- Cambiado de 128 a 124\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar con optimizador modificado\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # <-- Optimizer con tasa de aprendizaje √≥ptima\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Graficar p√©rdida\n",
    "plt.plot(history.history['loss'], label='P√©rdida entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='P√©rdida validaci√≥n')\n",
    "plt.xlabel('√âpoca')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.legend()\n",
    "plt.title('Convergencia de la funci√≥n de p√©rdida')\n",
    "plt.show()\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Reporte sklearn\n",
    "print(\"\\nReporte de clasificaci√≥n (precision, recall, f1-score):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusi√≥n:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
