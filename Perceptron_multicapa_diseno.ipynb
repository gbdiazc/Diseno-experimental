{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01275ef5-fe77-441d-82fc-2c2473972a0a",
   "metadata": {
    "id": "01275ef5-fe77-441d-82fc-2c2473972a0a"
   },
   "source": [
    "# Clasificación de MNIST con un Perceptrón Multicapa (MLP)\n",
    "\n",
    "## Perceptrón Multicapa (MLP)\n",
    "\n",
    "Un **Perceptrón Multicapa (MLP)** es una red neuronal artificial compuesta por varias capas de neuronas. Es uno de los modelos más básicos y clásicos de redes neuronales, utilizado para tareas de clasificación y regresión.\n",
    "\n",
    "###  Estructura del MLP:\n",
    "\n",
    "1. **Capa de entrada (Input layer):**\n",
    "   - Recibe los datos de entrada.\n",
    "   - En el caso del dataset MNIST, cada imagen tiene tamaño 28x28 píxeles, que se convierte en un vector de 784 elementos.\n",
    "   - Cada elemento representa un píxel con un valor entre 0 y 1 (después de normalizar).\n",
    "\n",
    "2. **Capas ocultas (Hidden layers):**\n",
    "   - Compuestas por neuronas densamente conectadas (fully connected).\n",
    "   - Usan funciones de activación no lineales (como ReLU).\n",
    "   - Permiten aprender representaciones internas complejas.\n",
    "\n",
    "3. **Capa de salida (Output layer):**\n",
    "   - Tiene 10 neuronas, una por cada clase (dígitos del 0 al 9).\n",
    "   - Utiliza activación **softmax** para generar una distribución de probabilidad.\n",
    "\n",
    "\n",
    "## Dataset MNIST?\n",
    "\n",
    "El dataset MNIST contiene:\n",
    "- 60,000 imágenes de entrenamiento y 10,000 de prueba.\n",
    "- Cada imagen es de 28x28 píxeles en escala de grises.\n",
    "- Las etiquetas son dígitos del 0 al 9.\n",
    "\n",
    "\n",
    "\n",
    "## Entrenamiento de un MLP\n",
    "\n",
    "El entrenamiento sigue un ciclo de pasos iterativos para ajustar los pesos de la red y minimizar el error en las predicciones.\n",
    "\n",
    "### 1. **Propagación hacia adelante (Forward pass)**\n",
    "\n",
    "Se calculan las salidas de cada capa de la red usando los pesos actuales:\n",
    "\n",
    "$$\n",
    "\\text{z}^{(1)} = W^{(1)}x + b^{(1)} \\quad \\text{→ entrada a la capa oculta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{a}^{(1)} = \\text{ReLU}(\\text{z}^{(1)}) \\quad \\text{→ salida de la capa oculta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{z}^{(2)} = W^{(2)}a^{(1)} + b^{(2)} \\quad \\text{→ entrada a la capa de salida}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z^{(2)}) \\quad \\text{→ probabilidades para cada clase}\n",
    "$$\n",
    "\n",
    "### 2. **Cálculo de la pérdida (Loss function)**\n",
    "\n",
    "Se compara la predicción del modelo con la etiqueta real mediante una función de pérdida. En clasificación multiclase, se usa la **entropía cruzada (cross-entropy)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{10} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $ y_i $ es la etiqueta real (en one-hot encoding).\n",
    "- $ \\hat{y}_i $ es la probabilidad predicha para la clase $i $.\n",
    "\n",
    "### 3. **Retropropagación y ajuste de pesos (Backpropagation + Gradient Descent)**\n",
    "\n",
    "Después de calcular la pérdida, necesitamos ajustar los pesos para reducir el error. Esto se hace en varios pasos:\n",
    "\n",
    "#### a) Error en la capa de salida\n",
    "\n",
    "El error que tiene la red en la salida es la diferencia entre la predicción y la etiqueta real:\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Aquí:\n",
    "- $\\delta^{(2)}$ es el vector de errores para la capa de salida.\n",
    "- $\\hat{y}$ es la salida predicha para cada clase.\n",
    "- $y$ es la etiqueta verdadera.\n",
    "\n",
    "#### b) Error en la capa oculta\n",
    "\n",
    "Este error se propaga hacia atrás para saber cuánto influyó cada neurona oculta en el error final:\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\circ \\text{ReLU}'(z^{(1)})\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $(W^{(2)})^T$ es la matriz de pesos de la capa de salida transpuesta.\n",
    "- $\\circ$ indica multiplicación elemento a elemento.\n",
    "- $\\text{ReLU}'(z^{(1)})$ es la derivada de ReLU, que vale 1 si $z^{(1)} > 0$, y 0 si no.\n",
    "\n",
    "#### c) Cálculo de gradientes para pesos y sesgos\n",
    "\n",
    "Con los errores calculados, ahora determinamos cuánto cambiar cada peso y sesgo para mejorar la predicción.\n",
    "\n",
    "- Gradiente para pesos de la capa de salida:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} = \\delta^{(2)} (a^{(1)})^T\n",
    "$$\n",
    "\n",
    "- Gradiente para sesgos de la capa de salida:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "- Gradiente para pesos de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\delta^{(1)} (x)^T\n",
    "$$\n",
    "\n",
    "- Gradiente para sesgos de la capa oculta:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "#### d) Actualización de pesos y sesgos\n",
    "\n",
    "Finalmente, ajustamos los pesos y sesgos usando la tasa de aprendizaje $\\eta$:\n",
    "\n",
    "$$\n",
    "W^{(l)} := W^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{(l)} := b^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{(l)}}\n",
    "$$\n",
    "\n",
    "con $l = 1, 2$ indicando la capa (oculta o salida).cto a ese peso.\n",
    "\n",
    "\n",
    "###  Entrenamiento por lotes y épocas\n",
    "\n",
    "- El conjunto de datos se divide en lotes (batches).\n",
    "- Cada lote se usa para actualizar los pesos una vez.\n",
    "- Una **época** es un recorrido completo por todo el conjunto de entrenamiento.\n",
    "- El modelo mejora iterativamente al ver más datos.\n",
    "\n",
    "\n",
    "\n",
    "###  ¿Qué aprende el modelo?\n",
    "\n",
    "- Comienza con pesos aleatorios.\n",
    "- A través del entrenamiento, ajusta los pesos para minimizar el error.\n",
    "- Aprende a asociar patrones visuales (píxeles) con etiquetas (números).\n",
    "- El objetivo es **generalizar bien** para predecir correctamente imágenes no vistas.\n",
    "\n",
    "\n",
    "Este proceso permite que un MLP aprenda a clasificar imágenes de dígitos escritos a mano con alta precisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f14fc-3808-4f76-a638-e008d3ce02d6",
   "metadata": {
    "id": "cf0f14fc-3808-4f76-a638-e008d3ce02d6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar píxeles\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "#  Learning rate\n",
    "learning_rate_opt = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_opt)\n",
    "\n",
    "#  Modelo MLP con 124 neuronas (óptimo)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar con optimizador modificado\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # <-- Optimizer con tasa de aprendizaje óptima\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Graficar pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.title('Convergencia de la función de pérdida')\n",
    "plt.show()\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Reporte sklearn\n",
    "print(\"\\nReporte de clasificación (precision, recall, f1-score):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0860e05-9650-4f23-bc0b-5fe0b21e3e16",
   "metadata": {
    "id": "a0860e05-9650-4f23-bc0b-5fe0b21e3e16"
   },
   "source": [
    "# Diseño de Experimentos para un Perceptrón Multicapa (MLP)\n",
    "\n",
    "## 1. Define el objetivo del experimento\n",
    "\n",
    "- Optimizar la arquitectura (número de capas, neuronas).\n",
    "- Optimizar hiperparámetros de entrenamiento (learning rate, batch size, regularización).\n",
    "- Comparar funciones de activación o tipos de optimizadores.\n",
    "\n",
    "## 2. Tipos de factores\n",
    "\n",
    "- **Continuos:** learning rate, momentum, dropout rate, número de neuronas (discreto con muchos niveles).\n",
    "- **Discretos:** número de capas, funciones de activación, optimizadores.\n",
    "- **Categóricos:** tipos de regularización, funciones de activación, optimizador.\n",
    "\n",
    "## 3. Diseños experimentales comunes para MLP\n",
    "\n",
    "| Método                        | Descripción                                               | Ventajas                                | Limitaciones                         |\n",
    "|-------------------------------|-----------------------------------------------------------|----------------------------------------|-------------------------------------|\n",
    "| **Búsqueda en malla (Grid Search)**        | Probar combinaciones en rejilla regular.                 | Fácil de implementar, sistemático.    | Costoso, explora pobremente el espacio. |\n",
    "| **Búsqueda aleatoria (Random Search)**     | Probar puntos aleatorios en el espacio.                   | Más eficiente que grid, fácil.         | No garantiza cobertura completa.   |\n",
    "| **Optimización bayesiana**                  | Modela la función objetivo con un modelo probabilístico. | Muy eficiente, menos ejecuciones.      | Más compleja, requiere librerías.   |\n",
    "| **Diseño de Superficie de Respuesta (RSM)**| Usa diseños factoriales y centrales para ajustar modelo. | Balance entre exploración y modelado.  | Requiere función respuesta suave, pocos factores. |\n",
    "| **Diseño factorial fraccional**             | Explora combinaciones seleccionadas para reducir pruebas.| Reduce experimentos, explora interacciones principales. | Menos exhaustivo, puede perder interacciones. |\n",
    "| **Algoritmos heurísticos**             | Son una combinación entre exploración y explotación del espacio.| Puede explorar mas parte del espacio. | En ocasiones puede ser costoso computacionalmente. |\n",
    "\n",
    "## 4. Recomendaciones prácticas\n",
    "\n",
    "- Para pocos factores continuos: **RSM con diseño Central Compuesto (CCD)** para entender efectos e interacciones.\n",
    "- Para muchos hiperparámetros o combinaciones: **búsqueda aleatoria o bayesiana**.\n",
    "- Usar siempre conjunto de validación para medir desempeño.\n",
    "- Realizar réplicas en condiciones centrales para estimar variabilidad.\n",
    "\n",
    "## 5. Variables recomendadas a experimentar en un MLP\n",
    "\n",
    "- Learning rate (continuo)\n",
    "- Número de neuronas por capa (discreto)\n",
    "- Número de capas (discreto)\n",
    "- Batch size (discreto)\n",
    "- Regularización (L2 lambda, dropout rate)\n",
    "- Función de activación (ReLU, tanh, sigmoid)\n",
    "\n",
    "## 6. Ejemplo óptimo para un MLP simple\n",
    "\n",
    "- Para optimizar learning rate, neuronas y batch size: **Diseño RSM** es ideal.\n",
    "- Para probar arquitecturas y optimizadores: combinar diseño factorial fraccional (variables categóricas) y RSM o búsqueda aleatoria (variables continuas).\n",
    "\n",
    "\n",
    "\n",
    "## Resumen: ¿Cuál es la mejor forma?\n",
    "\n",
    "- **Pocos factores numéricos:** Diseño Central Compuesto (CCD) con RSM.\n",
    "- **Muchos hiperparámetros:** Optimización bayesiana o búsqueda aleatoria.\n",
    "- **Experimentos rápidos:** Grid search con validación cruzada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f380584-125c-43b8-9748-05598acb85fa",
   "metadata": {
    "id": "0f380584-125c-43b8-9748-05598acb85fa"
   },
   "source": [
    "# Diseño Central Compuesto (CCD) y Método de Superficie de Respuesta (RSM)\n",
    "\n",
    "## Introducción al Método de Superficie de Respuesta (RSM)\n",
    "\n",
    "El Método de Superficie de Respuesta (Response Surface Methodology, RSM) es un conjunto de técnicas estadísticas para modelar y analizar problemas en los cuales una o más variables de respuesta $y$ dependen de varias variables independientes o factores $x_1, x_2, ..., x_k$.\n",
    "\n",
    "El objetivo principal del RSM es:\n",
    "\n",
    "- Encontrar las condiciones óptimas de los factores que maximicen o minimicen la respuesta.\n",
    "- Modelar la relación entre las variables de entrada y la respuesta mediante funciones matemáticas simples, generalmente polinomios de segundo grado (cuadráticos).\n",
    "\n",
    "\n",
    "##  Diseño Central Compuesto (CCD)\n",
    "\n",
    "El Diseño Central Compuesto (Central Composite Design, CCD) es uno de los diseños experimentales más usados para ajustar modelos de superficie de respuesta de segundo orden (cuadráticos). Es ideal cuando se tienen pocos factores numéricos y se desea explorar no solo los efectos lineales sino también los efectos cuadráticos y las interacciones entre factores.\n",
    "\n",
    "### Componentes del CCD:\n",
    "\n",
    "- **Puntos factoriales:** Combinaciones de los niveles altos (+1) y bajos (-1) para cada factor, formando un diseño factorial completo ($2^k$ puntos para $k$ factores).\n",
    "- **Puntos axiales (puntos estrella):** Extienden el rango de exploración fuera del diseño factorial con niveles a distancia $\\pm \\alpha$ en cada factor, manteniendo los demás en nivel central (0). Estos puntos permiten estimar efectos cuadráticos.\n",
    "- **Puntos centrales:** Uno o más puntos en el centro del diseño (nivel 0 para todos los factores) que ayudan a estimar la variabilidad experimental y la curvatura.\n",
    "\n",
    "### Parámetro $\\alpha$:\n",
    "\n",
    "El valor de $\\alpha$ define la distancia de los puntos axiales al centro y puede elegirse para mantener propiedades estadísticas específicas, como la rotabilidad (igual precisión en todas las direcciones).\n",
    "\n",
    "\n",
    "\n",
    "## Codificación de factores\n",
    "\n",
    "Para facilitar el análisis, los niveles de los factores se codifican de forma estandarizada:\n",
    "\n",
    "$$\n",
    "x_i = \\frac{\\text{valor real} - \\text{valor central}}{\\text{semi-rango}}\n",
    "$$\n",
    "donde:\n",
    "\n",
    "- $x_i$ es el nivel codificado del factor $i$, típicamente en el rango $[-1, +1]$ para los puntos factoriales.\n",
    "- El valor central es el punto medio del rango real del factor.\n",
    "- El semi-rango es la mitad del rango real.\n",
    "\n",
    "Esta codificación permite comparar efectos de diferentes factores en una escala común.\n",
    "\n",
    "\n",
    "\n",
    "##  Ajuste del modelo de superficie de respuesta\n",
    "\n",
    "Con los datos obtenidos del experimento CCD, se ajusta un modelo polinomial cuadrático de la forma:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\sum_{i=1}^k \\beta_i x_i + \\sum_{i=1}^k \\beta_{ii} x_i^2 + \\sum_{i<j} \\beta_{ij} x_i x_j + \\varepsilon\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $y$ es la variable respuesta (p. ej., precisión del modelo MLP).\n",
    "- $\\beta_0$ es la ordenada al origen.\n",
    "- $\\beta_i$, $\\beta_{ii}$, y $\\beta_{ij}$ son los coeficientes que representan los efectos lineales, cuadráticos e interacción, respectivamente.\n",
    "- $\\varepsilon$ es el término de error experimental.\n",
    "\n",
    "Este modelo permite:\n",
    "\n",
    "- Identificar los efectos significativos de cada factor y sus interacciones.\n",
    "- Visualizar la superficie de respuesta.\n",
    "- Encontrar los valores óptimos de los factores que maximizan o minimizan $y$.\n",
    "\n",
    "\n",
    "##  Aplicación práctica en MLP\n",
    "\n",
    "En el contexto de un Perceptrón Multicapa (MLP):\n",
    "\n",
    "- Los factores pueden ser hiperparámetros como tasa de aprendizaje, número de neuronas, número de capas, etc.\n",
    "- La respuesta puede ser la precisión, error de validación o cualquier métrica de desempeño.\n",
    "- Utilizando CCD y RSM, podemos diseñar un conjunto reducido pero eficiente de experimentos para explorar cómo los hiperparámetros afectan el desempeño del MLP y encontrar configuraciones óptimas sin realizar búsquedas exhaustivas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72c452-5ee0-439f-979c-4eaea8434cd4",
   "metadata": {
    "id": "de72c452-5ee0-439f-979c-4eaea8434cd4"
   },
   "source": [
    "# Optimización de un Perceptrón Multicapa (MLP) con Superficie de Respuesta (RSM) y Diseño Central Compuesto (CCD)\n",
    "\n",
    "##  Objetivo\n",
    "\n",
    "Optimizar el rendimiento de un Perceptrón Multicapa (MLP) ajustando dos hiperparámetros clave:\n",
    "\n",
    "- **Tasa de aprendizaje** (`learning_rate`)\n",
    "- **Número de neuronas ocultas** (`units`)\n",
    "\n",
    "Usamos el **Método de Superficie de Respuesta (RSM)** con un **Diseño Central Compuesto (CCD)** para construir un modelo cuadrático que relacione estos hiperparámetros con el desempeño del MLP (precisión de validación).\n",
    "\n",
    "\n",
    "##  Paso 1: Selección de factores y niveles\n",
    "\n",
    "Seleccionamos dos factores, cada uno con 5 niveles: codificados como $- \\alpha, -1, 0, +1, +\\alpha$, donde $\\alpha = \\sqrt{2} \\approx 1.414$ (valor común para dos factores).\n",
    "\n",
    "###  Factores y niveles reales\n",
    "\n",
    "| Factor              | $-\\alpha$  | $-1$      | $0$       | $+1$      | $+\\alpha$  |\n",
    "|---------------------|------------|-----------|-----------|-----------|------------|\n",
    "| Learning rate       | 0.00001    | 0.0001    | 0.001     | 0.01      | 0.1        |\n",
    "| Neuronas ocultas    | 32         | 64        | 128       | 256       | 512        |\n",
    "\n",
    "###  Puntos del diseño CCD\n",
    "\n",
    "El diseño CCD incluye:\n",
    "\n",
    "- **4 puntos factoriales**: combinaciones de niveles $-1$ y $+1$\n",
    "- **4 puntos axiales**: niveles extremos $- \\alpha$ y $+ \\alpha$ con los demás factores en 0\n",
    "- **5 réplicas del punto central**: todos los factores en nivel 0\n",
    "\n",
    "Esto da un total de **13 experimentos**.\n",
    "\n",
    "| Punto | $x_1$ (codificado) | $x_2$ (codificado) | learning\\_rate | units |\n",
    "|-------|--------------------|--------------------|----------------|--------|\n",
    "| F1    | -1                 | -1                 | 0.0001         | 64     |\n",
    "| F2    | +1                 | -1                 | 0.01           | 64     |\n",
    "| F3    | -1                 | +1                 | 0.0001         | 256    |\n",
    "| F4    | +1                 | +1                 | 0.01           | 256    |\n",
    "| A1    | -1.414             | 0                  | 0.00004        | 128    |\n",
    "| A2    | +1.414             | 0                  | 0.0252         | 128    |\n",
    "| A3    | 0                  | -1.414             | 0.001          | 45     |\n",
    "| A4    | 0                  | +1.414             | 0.001          | 362    |\n",
    "| C1    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C2    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C3    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C4    | 0                  | 0                  | 0.001          | 128    |\n",
    "| C5    | 0                  | 0                  | 0.001          | 128    |\n",
    "\n",
    "##  Paso 2: Modelo cuadrático de respuesta\n",
    "\n",
    "Usamos un modelo cuadrático de segundo orden:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $y$ es la precisión de validación del MLP\n",
    "- $x_1$: nivel codificado del `learning_rate`\n",
    "- $x_2$: nivel codificado del número de `units`\n",
    "- $\\beta$ son los coeficientes a estimar\n",
    "- $\\varepsilon$ es el error aleatorio\n",
    "\n",
    "Este modelo permite capturar efectos lineales, cuadráticos y de interacción entre los hiperparámetros.\n",
    "\n",
    "\n",
    "## Paso 3: Ejecución del experimento\n",
    "\n",
    "1. **Construir la matriz de diseño CCD** con los niveles codificados ($x_1$, $x_2$).\n",
    "2. **Convertir los niveles codificados** a valores reales de hiperparámetros.\n",
    "3. **Entrenar el MLP** con cada combinación y registrar la precisión de validación.\n",
    "4. **Ajustar el modelo cuadrático** usando regresión lineal (con `statsmodels` o `sklearn.linear_model.LinearRegression`).\n",
    "5. **Analizar los resultados**:\n",
    "   - Significancia de los términos (lineales, cuadráticos, interacción)\n",
    "   - Superficie de respuesta (mapa 3D o contornos)\n",
    "   - Hiperparámetros óptimos\n",
    "\n",
    "\n",
    "## Ventajas del enfoque\n",
    "\n",
    "- Eficiencia: Menos combinaciones que una búsqueda en malla (grid search).\n",
    "- Modelo explicativo y predictivo del desempeño del MLP.\n",
    "- Posibilidad de visualizar e interpretar la superficie de respuesta.\n",
    "- Permite evaluar **efectos individuales y combinados** de los hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a72d3a1-0bd1-4dd2-a84a-10d3d6df116a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a72d3a1-0bd1-4dd2-a84a-10d3d6df116a",
    "outputId": "2189a016-2a22-45d7-e49d-d1a8b835f6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Ejecutando experimento 1: lr=0.00010, units=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando experimento 2: lr=0.01000, units=64\n",
      "Ejecutando experimento 3: lr=0.00010, units=256\n",
      "Ejecutando experimento 4: lr=0.01000, units=256\n",
      "Ejecutando experimento 5: lr=0.00004, units=128\n",
      "Ejecutando experimento 6: lr=0.02594, units=128\n",
      "Ejecutando experimento 7: lr=0.00100, units=48\n",
      "Ejecutando experimento 8: lr=0.00100, units=341\n",
      "Ejecutando experimento 9: lr=0.00100, units=128\n",
      "Ejecutando experimento 10: lr=0.00100, units=128\n",
      "Ejecutando experimento 11: lr=0.00100, units=128\n",
      "Ejecutando experimento 12: lr=0.00100, units=128\n",
      "Ejecutando experimento 13: lr=0.00100, units=128\n",
      "       x1     x2  learning_rate  units  val_accuracy\n",
      "0  -1.000 -1.000       0.000100     64      0.940417\n",
      "1   1.000 -1.000       0.010000     64      0.966667\n",
      "2  -1.000  1.000       0.000100    256      0.960917\n",
      "3   1.000  1.000       0.010000    256      0.967417\n",
      "4  -1.414  0.000       0.000039    128      0.929833\n",
      "5   1.414  0.000       0.025942    128      0.943750\n",
      "6   0.000 -1.414       0.001000     48      0.966750\n",
      "7   0.000  1.414       0.001000    341      0.981667\n",
      "8   0.000  0.000       0.001000    128      0.975917\n",
      "9   0.000  0.000       0.001000    128      0.974833\n",
      "10  0.000  0.000       0.001000    128      0.976083\n",
      "11  0.000  0.000       0.001000    128      0.973833\n",
      "12  0.000  0.000       0.001000    128      0.974917\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar y normalizar los datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Dividir entrenamiento en entrenamiento + validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Función de entrenamiento\n",
    "# -----------------------------\n",
    "def train_model(learning_rate, units):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Devolver última precisión de validación\n",
    "    return history.history['val_accuracy'][-1]\n",
    "\n",
    "# -----------------------------\n",
    "# Tabla del diseño CCD\n",
    "# -----------------------------\n",
    "design = pd.DataFrame([\n",
    "    [-1, -1], [1, -1], [-1, 1], [1, 1],     # factorial\n",
    "    [-1.414, 0], [1.414, 0], [0, -1.414], [0, 1.414],  # axial\n",
    "    [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]  # centro (5 rep)\n",
    "], columns=[\"x1\", \"x2\"])\n",
    "\n",
    "# Conversión a hiperparámetros reales\n",
    "def coded_to_real(x1, x2):\n",
    "    # learning_rate en escala log10\n",
    "    learning_rate = 0.001 * (10 ** x1)\n",
    "    units = int(round(128 * (2 ** x2)))\n",
    "    return learning_rate, units\n",
    "\n",
    "# -----------------------------\n",
    "# Entrenamiento en cada punto\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for i, row in design.iterrows():\n",
    "    x1, x2 = row[\"x1\"], row[\"x2\"]\n",
    "    lr, u = coded_to_real(x1, x2)\n",
    "\n",
    "    print(f\"Ejecutando experimento {i+1}: lr={lr:.5f}, units={u}\")\n",
    "    acc = train_model(lr, u)\n",
    "\n",
    "    results.append({\n",
    "        \"x1\": x1,\n",
    "        \"x2\": x2,\n",
    "        \"learning_rate\": lr,\n",
    "        \"units\": u,\n",
    "        \"val_accuracy\": acc\n",
    "    })\n",
    "\n",
    "# Convertir a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee0312-ec0e-45b5-bb25-64a2ad0ba03b",
   "metadata": {
    "id": "81ee0312-ec0e-45b5-bb25-64a2ad0ba03b"
   },
   "source": [
    "## Paso 4: Ajuste del modelo cuadrático de segundo orden\n",
    "Una vez entrenado el perceptrón para cada combinación de hiperparámetros del diseño CCD y registrada la precisión de validación, queremos ajustar un **modelo de segundo orden** que relacione los factores con la respuesta.\n",
    "\n",
    "El modelo que queremos ajustar es:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2 + \\varepsilon\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $y$ es la **precisión de validación**.\n",
    "- $x_1$ es el **nivel codificado del learning rate**.\n",
    "- $x_2$ es el **nivel codificado del número de unidades**.\n",
    "- Los $\\beta$ son los coeficientes que se estimarán.\n",
    "- $\\varepsilon$ es el error aleatorio (ruido experimental).\n",
    "\n",
    "- **$\\beta_0$**: intercepto (respuesta en el punto central).\n",
    "- **$\\beta_1$ y $\\beta_2$**: efectos lineales de los factores.\n",
    "- **$\\beta_{11}$ y $\\beta_{22}$**: efectos cuadráticos (curvatura) de cada factor.\n",
    "- **$\\beta_{12}$**: interacción entre factores (cómo cambia el efecto de un factor cuando el otro también cambia).\n",
    "\n",
    "\n",
    "###  Procedimiento :\n",
    "\n",
    "1. **Codificación de niveles**:\n",
    "   - Convertimos cada combinación real de hiperparámetros (learning rate, unidades) a sus niveles codificados: $x_1, x_2$.\n",
    "   - Esto se hace con la fórmula:\n",
    "\n",
    "   $$\n",
    "   x_i = \\frac{z_i - z_{i,0}}{\\Delta z_i}\n",
    "   $$\n",
    "\n",
    "   donde $z_i$ es el valor real del factor, $z_{i,0}$ es el valor central, y $\\Delta z_i$ es el paso entre niveles (diferencia entre centro y alto o centro y bajo).\n",
    "\n",
    "2. **Creación de variables del modelo**:\n",
    "   - A partir de los niveles codificados, construimos las variables del modelo: $x_1$, $x_2$, $x_1^2$, $x_2^2$, y $x_1x_2$.\n",
    "\n",
    "3. **Ajuste con regresión**:\n",
    "   - Usamos el paquete `statsmodels` para ajustar un modelo de regresión lineal múltiple con estos términos como predictores y la precisión como variable respuesta.\n",
    "\n",
    "4. **Evaluación del modelo**:\n",
    "   - Obtenemos los coeficientes estimados $\\hat{\\beta}$.\n",
    "   - Evaluamos la significancia estadística (p-valores) y el coeficiente de determinación ($R^2$) para ver qué tan bien ajusta el modelo.\n",
    "\n",
    "5. **Uso del modelo**:\n",
    "   - El modelo se puede usar para:\n",
    "     - Predecir la precisión esperada dados nuevos valores de hiperparámetros (dentro del rango).\n",
    "     - Visualizar la **superficie de respuesta**.\n",
    "     - Encontrar la combinación óptima que maximiza la precisión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d350bec7-3ec0-429a-9506-e978cebe20bf",
   "metadata": {
    "id": "d350bec7-3ec0-429a-9506-e978cebe20bf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m product\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1. Definir niveles reales y codificados\n",
    "center = {'lr': 0.001, 'units': 128}\n",
    "step = {'lr': 0.0009, 'units': 64}  # (alto - centro)\n",
    "alpha = 1.414  # valor axial\n",
    "\n",
    "# Niveles codificados para CCD\n",
    "ccd_points = [\n",
    "    [-1, -1], [-1,  1], [1, -1], [1,  1],  # factorial\n",
    "    [-alpha, 0], [alpha, 0], [0, -alpha], [0, alpha],  # axiales\n",
    "    [0, 0], [0, 0], [0, 0]  # 3 repeticiones del centro\n",
    "]\n",
    "\n",
    "# Convertimos a hiperparámetros reales\n",
    "def decode_level(lr_lvl, unit_lvl):\n",
    "    lr_real = center['lr'] + lr_lvl * step['lr']\n",
    "    units_real = int(center['units'] + unit_lvl * step['units'])\n",
    "    return lr_real, units_real\n",
    "\n",
    "real_hyperparams = [decode_level(lr, u) for lr, u in ccd_points]\n",
    "\n",
    "# 2. Cargar y normalizar MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_train[-12000:]\n",
    "y_val = y_train[-12000:]\n",
    "x_train = x_train[:-12000]\n",
    "y_train = y_train[:-12000]\n",
    "\n",
    "# 3. Entrenar modelo para cada combinación\n",
    "accuracies = []\n",
    "for i, (lr, units) in enumerate(real_hyperparams):\n",
    "    print(f\"🔁 Iteración {i+1}: learning_rate={lr}, units={units}\")\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "    loss, acc = model.evaluate(x_val, y_val, verbose=0)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "# 4. Construir variables del modelo cuadrático\n",
    "design_df = pd.DataFrame(ccd_points, columns=['x1', 'x2'])  # niveles codificados\n",
    "design_df['x1^2'] = design_df['x1'] ** 2\n",
    "design_df['x2^2'] = design_df['x2'] ** 2\n",
    "design_df['x1*x2'] = design_df['x1'] * design_df['x2']\n",
    "design_df['accuracy'] = accuracies\n",
    "\n",
    "# 5. Ajustar modelo con statsmodels\n",
    "X = design_df[['x1', 'x2', 'x1^2', 'x2^2', 'x1*x2']]\n",
    "X = sm.add_constant(X)\n",
    "y = design_df['accuracy']\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775c31b-71a7-42a5-8367-224e9e953594",
   "metadata": {
    "id": "b775c31b-71a7-42a5-8367-224e9e953594"
   },
   "source": [
    "### Evaluación global del modelo\n",
    "\n",
    "| Métrica               | Valor      | Interpretación |\n",
    "|------------------------|------------|----------------|\n",
    "| **R-squared**          | 0.628      | El modelo explica el **62.8% de la variabilidad** observada en la precisión. Es aceptable pero no excelente. |\n",
    "| **Adj. R-squared**     | 0.255      | Al ajustar por el número de predictores, la varianza explicada baja a **25.5%**, indicando que algunos términos podrían **no contribuir significativamente**. |\n",
    "| **F-statistic**        | 1.685      | La prueba F evalúa si el modelo completo es mejor que uno sin predictores. |\n",
    "| **Prob(F-statistic)**  | 0.291      | No significativo (p > 0.05). **No hay evidencia estadística de que el modelo en conjunto sea útil**. |\n",
    "| **N° Observaciones**   | 11         | El tamaño muestral es pequeño, lo que puede reducir la precisión de los coeficientes estimados. |\n",
    "\n",
    "\n",
    "### Interpretación de coeficientes\n",
    "\n",
    "El modelo ajustado fue:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\n",
    "$$\n",
    "\n",
    "| Término     | Coef. | p-valor | Interpretación |\n",
    "|-------------|-------|---------|----------------|\n",
    "| **const**   | 0.971 | 0.001   | Precisión esperada en el punto central del diseño. Muy significativa. |\n",
    "| **$x_1$** (lineal) | 0.161 | 0.097   | Efecto lineal positivo del learning rate. Aunque no es significativo (p > 0.05), es el más cercano a serlo. |\n",
    "| **$x_2$** (lineal) | 0.0058 | 0.944  | El número de neuronas ocultas **no tiene efecto lineal significativo**. |\n",
    "| **$x_1^2$** (cuadrático) | -0.165 | 0.138  | Indica curvatura descendente: puede existir un máximo de precisión para un valor intermedio de $x_1$, pero no es concluyente. |\n",
    "| **$x_2^2$** (cuadrático) | 0.0475 | 0.634  | No hay evidencia de curvatura respecto al número de neuronas ocultas. |\n",
    "| **$x_1 \\cdot x_2$** (interacción) | -0.0034 | 0.977 | La interacción entre factores es prácticamente nula. |\n",
    "\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- **El único efecto con cierto impacto es el término lineal de $x_1$ (learning rate)**, pero no alcanza significancia estadística.\n",
    "- **El número de neuronas ocultas no parece influir significativamente** en la precisión del MLP, al menos dentro del rango analizado.\n",
    "- **No se detecta interacción significativa** entre ambos factores.\n",
    "- El modelo puede **beneficiarse de simplificación** (eliminando términos no significativos), o de **mayor cantidad de datos** (más réplicas o puntos del diseño) para mejorar la confiabilidad.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f78e4f-8d37-4e03-9062-0b1ebd12ca62",
   "metadata": {
    "id": "20f78e4f-8d37-4e03-9062-0b1ebd12ca62"
   },
   "source": [
    "## Derivación del Punto Óptimo de la Superficie de Respuesta\n",
    "\n",
    "Queremos encontrar el punto donde la precisión del MLP es máxima, usando el modelo cuadrático ajustado mediante el Método de Superficie de Respuesta (RSM). El modelo ajustado es:\n",
    "\n",
    "$$\n",
    "y(x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12} x_1 x_2\n",
    "$$\n",
    "\n",
    "### Modelo con coeficientes ajustados\n",
    "\n",
    "Sustituimos los coeficientes obtenidos del ajuste del modelo (OLS):\n",
    "\n",
    "$$\n",
    "y(x_1, x_2) = 0.9714 + 0.1607 x_1 + 0.0058 x_2 - 0.1655 x_1^2 + 0.0475 x_2^2 - 0.0034 x_1 x_2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Derivadas parciales\n",
    "\n",
    "Para encontrar el punto óptimo, derivamos parcialmente respecto a $x_1$ y $x_2$, e igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = \\beta_1 + 2\\beta_{11} x_1 + \\beta_{12} x_2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_2} = \\beta_2 + 2\\beta_{22} x_2 + \\beta_{12} x_1 = 0\n",
    "$$\n",
    "\n",
    "Sustituimos los valores numéricos:\n",
    "\n",
    "**Ecuación 1:**\n",
    "\n",
    "$$\n",
    "0.1607 - 2(0.1655) x_1 - 0.0034 x_2 = 0\n",
    "$$\n",
    "\n",
    "**Ecuación 2:**\n",
    "\n",
    "$$\n",
    "0.0058 + 2(0.0475) x_2 - 0.0034 x_1 = 0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Sistema de ecuaciones lineales\n",
    "\n",
    "Simplificamos:\n",
    "\n",
    "1. $-0.331 x_1 - 0.0034 x_2 = -0.1607$  \n",
    "2. $-0.0034 x_1 + 0.095 x_2 = -0.0058$\n",
    "\n",
    "\n",
    "\n",
    "### Solución del sistema\n",
    "\n",
    "Resolviendo el sistema se obtiene:\n",
    "\n",
    "- $x_1^* = 0.486$  \n",
    "- $x_2^* = -0.044$\n",
    "\n",
    "Este es el **punto óptimo en coordenadas codificadas**.\n",
    "\n",
    "\n",
    "###  Conversión a valores reales\n",
    "\n",
    "Usamos la fórmula de transformación de coordenadas codificadas a reales:\n",
    "\n",
    "$$\n",
    "x_{\\text{real}} = x_{\\text{center}} + x^* \\cdot \\frac{x_{\\text{high}} - x_{\\text{low}}}{2}\n",
    "$$\n",
    "\n",
    "- Para **learning rate** ($x_1$):  \n",
    "  $0.001 + 0.486 \\cdot \\frac{0.01 - 0.0001}{2} \\approx 0.0034$\n",
    "\n",
    "- Para **units** ($x_2$):  \n",
    "  $128 + (-0.044) \\cdot \\frac{256 - 64}{2} \\approx 124$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc6292-c4e2-4308-b799-23531f7ab80c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54dc6292-c4e2-4308-b799-23531f7ab80c",
    "outputId": "781d2405-e29a-4cb9-f528-8ac4c96083d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 - 6s - 7ms/step - accuracy: 0.9268 - loss: 0.2430 - val_accuracy: 0.9560 - val_loss: 0.1526\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalizar píxeles\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "#  Learning rate óptimo\n",
    "learning_rate_opt = 0.0034\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_opt)\n",
    "\n",
    "#  Modelo MLP con 124 neuronas (óptimo)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(124, activation='relu'),  # <-- Cambiado de 128 a 124\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar con optimizador modificado\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # <-- Optimizer con tasa de aprendizaje óptima\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Graficar pérdida\n",
    "plt.plot(history.history['loss'], label='Pérdida entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "plt.title('Convergencia de la función de pérdida')\n",
    "plt.show()\n",
    "\n",
    "# Evaluar en test\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Reporte sklearn\n",
    "print(\"\\nReporte de clasificación (precision, recall, f1-score):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
